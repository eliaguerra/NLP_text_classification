{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estrazione dei prompt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/eliaguerra/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import matplotlib.pyplot as plt\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = [\"sci.space\", \"rec.autos\", \"rec.sport.hockey\"] # Categories extracted from the 20newsgroups dataset\n",
    "k = 4 # Number of sentences to be extracted from 20newsgroups dataset and used as prompts\n",
    "min_number_of_words = 20 # Minimum number of words in a sentence to be considered as a prompt\n",
    "random_seed = 0 # Random seed for reproducibility"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il dataset viene caricando rimuovendo le parti relative a `headers`, `footers` e `quotes`. I post sono contenuti in `dataset.data`. La divisione in frasi viene fatta tramite la libreria `nltk` e il tokenizzatore `punkt` tramite `sent_tokenize`. I prompt vengono salvati con i rispettivi argomenti su un Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = fetch_20newsgroups(subset=\"all\", remove=(\"headers\", \"footers\", \"quotes\"), categories=categories, shuffle=False) # 20newsgroups dataset loading\n",
    "first_k_sentences = [' '.join(sent_tokenize(sample)[:k]) for sample in dataset.data] # Extracting the first k sentences from each sample \n",
    "df = pd.DataFrame(first_k_sentences, columns=['First k Sentences']) # Dataframe creation\n",
    "df['Label'] = dataset.target\n",
    "df['label_name'] = df['Label'].apply(lambda x: dataset.target_names[x]) # Adding the label names to the dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calcolo del numero di parole per ogni prompt ed eliminazione dei prompt con meno di `min_number_of_words` parole"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_words(sentence):\n",
    "    # Simple function to count the number of words in a sentence\n",
    "    return len(sentence.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average number of words:  71.01012066952121\n",
      "Standard deviation:  161.99423057212127\n",
      "Maximum number of words:  6864\n",
      "Minimum number of words:  21\n"
     ]
    }
   ],
   "source": [
    "df['Number of Words'] = df['First k Sentences'].apply(count_words)\n",
    "df = df[df['Number of Words'] > min_number_of_words] # Filtering sentences with less than min_number_of_words words\n",
    "print(\"Average number of words: \", df['Number of Words'].mean())\n",
    "print(\"Standard deviation: \", df['Number of Words'].std())\n",
    "print(\"Maximum number of words: \", df['Number of Words'].max())\n",
    "print(\"Minimum number of words: \", df['Number of Words'].min())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calcolo del numero di prompt per ogni argomento. Al fine di bilanciare il dataset, il numero di prompt per ogni argomento viene al numero di prompt dell'argomento con cardinaliÃ  minore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label\n",
      "2    875\n",
      "1    862\n",
      "0    832\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "label_counts = df['Label'].value_counts() # Counting the number of samples for each label\n",
    "print(label_counts)\n",
    "class_samples = min(label_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Selezione randomica dei prompt per ogni argomento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label\n",
      "0    832\n",
      "1    832\n",
      "2    832\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "dfs = []\n",
    "for label in [0,1,2]:\n",
    "    filtered_df = df[df['Label'] == label]\n",
    "\n",
    "    if len(filtered_df) >= class_samples:\n",
    "        sampled_df = filtered_df.sample(class_samples, random_state=random_seed) # Sampling class_samples samples from each category\n",
    "        dfs.append(sampled_df)\n",
    "    else:\n",
    "        print(f\"Category {label} has less than {class_samples} samples\")\n",
    "result_df = pd.concat(dfs, ignore_index=True)\n",
    "print(result_df['Label'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creazione del dataset sintetico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "from tqdm.notebook import tqdm_notebook\n",
    "import torch\n",
    "import re\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\" # Checking if GPU is available\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2-large\") # Loading the GPT-2 tokenizer\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2-large\").to(device) # Load the pre-trained GPT-2 model\n",
    "min_new_tokens = 20 # Minimum number of tokens to be generated\n",
    "max_new_tokens = 300 # Maximum number of tokens to be generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_string(text):\n",
    "    \"\"\"\n",
    "    Removes unwanted characters.\n",
    "\n",
    "    Parameters:\n",
    "        text (str): The input string to be processed.\n",
    "\n",
    "    Returns:\n",
    "        str: The processed string with unwanted characters removed and formatted.\n",
    "    \"\"\"\n",
    "    text = re.sub(r\"[^\\w\\s.,!?']\", ' ', text) # Replace all characters that are not words, spaces, periods, commas, exclamation marks, question marks, or apostrophes with a space\n",
    "    text = re.sub('[\\n\\t]+', ' ', text) # Replace all newlines and tabs with a space\n",
    "    text = re.sub(' +', ' ', text) # Replace all multiple spaces with a single space\n",
    "    text = ''.join(c for c in text if c.isprintable()) # Remove non-printable characters\n",
    "    return text.strip()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_dataset(input, tokenizer, model, min_new_tok, max_new_tok, device = \"cpu\"):\n",
    "    \"\"\"\n",
    "    Generates samples from the input dataset using a given model. The function skips the generation process if the length of the encoded input and max_new_tok exceeds 1024.\n",
    "\n",
    "    Parameters:\n",
    "        input (pandas.DataFrame): The input dataset. It should have columns 'First k Sentences', 'Label', and 'label_name'.\n",
    "        tokenizer (PreTrainedTokenizer): The tokenizer to be used for encoding the prompts.\n",
    "        model (PreTrainedModel): The model to be used for generating the samples.\n",
    "        min_new_tok (int): The minimum number of new tokens to generate.\n",
    "        max_new_tok (int): The maximum number of new tokens to generate.\n",
    "        device (str, optional): The device to run the model on. Defaults to \"cpu\".\n",
    "\n",
    "    Returns:\n",
    "        tuple: Two lists of dictionaries. The first list contains the raw generated samples with the keys 'label', 'label_name', 'prompt', and 'text'. \n",
    "        The second list contains the processed generated samples with the same keys.\n",
    "    \"\"\"\n",
    "    generated_samples_raw = []\n",
    "    generated_samples = []\n",
    "    for idx, input_row in tqdm_notebook(input.iterrows(), total = input.shape[0]):\n",
    "        prompt = process_string(input_row['First k Sentences']) # Process the input prompt with the function previously defined\n",
    "        label = input_row['Label']\n",
    "        label_name = input_row['label_name']\n",
    "        encoded_input = tokenizer.encode(prompt, return_tensors=\"pt\").to(device) # Encode the input prompt\n",
    "\n",
    "        if len(encoded_input[0]) + max_new_tok > 1024: # Skip the generation process if it exceeds 1024 tokens otherwise it will raise an error\n",
    "            continue\n",
    "\n",
    "        outputs = model.generate(encoded_input,  # Encoded prompt\n",
    "                                 min_new_tokens = min_new_tok, # Min number of new tokens\n",
    "                                 max_new_tokens = max_new_tok, # Max number of new tokens\n",
    "                                 num_return_sequences=1, # Number of samples to generate\n",
    "                                 no_repeat_ngram_size=2, # To avoid word repetition\n",
    "                                 pad_token_id=50256, \n",
    "                                 eos_token_id=50256\n",
    "                                 )\n",
    "        data = tokenizer.decode(outputs[0], skip_special_tokens=True, clean_up_tokenization_spaces=True) # Decode the output\n",
    "        generated_samples_raw.append({\"label\": label, \"label_name\": label_name, \"prompt\": prompt, \"text\": data})\n",
    "        generated_samples.append({\"label\": label, \"label_name\": label_name, \"prompt\": prompt, \"text\": process_string(data)})\n",
    "    return generated_samples_raw, generated_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generazione del dataset sintetico eseguendo la funzione `generate_dataset` sul dataframe dei prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34867a2d01fa4f3cbcf3e4fa4e74ae8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "synthetic_dataset_raw, synthetic_dataset = generate_dataset(result_df, tokenizer, model, min_new_tokens, max_new_tokens, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esempio di output non post-processato"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'label': 1,\n",
       " 'label_name': 'rec.sport.hockey',\n",
       " 'prompt': 'The Selke candidate forwards main purpose on a shift is to prevent goals from being scored not to score them. When Lemieux or Gilmour play their number one purpose is to score defence is secondary especially considering the line that plays against them is probably a defensive one. That is why they are not Selke candidates. Someone posted something about this assumption being lost in translation it was a few months ago .',\n",
       " 'text': \"The Selke candidate forwards main purpose on a shift is to prevent goals from being scored not to score them. When Lemieux or Gilmour play their number one purpose is to score defence is secondary especially considering the line that plays against them is probably a defensive one. That is why they are not Selke candidates. Someone posted something about this assumption being lost in translation it was a few months ago. I think it is a good idea to look at the numbers and see if it holds true.\\n\\nThe first thing to note is that the Selkes are a very small sample size. The last time the Canucks were in the playoffs they had a total of 5 players on the team that were Selkendigs. This year they have only had 3. So it would be a stretch to say that they would have a higher percentage of Selkie's than the last few years. However, the fact that there are only 3 Selkies in this group is not a huge surprise. It is also not surprising that only one of them was on an NHL team. There are many reasons why a player would not be on their team, but the most common is because they were injured. In the case of the three players that are on this list, they all played in a playoff game. They were all injured and were not able to play in their playoff games. If you look back at their numbers, you will see that all three of these players were on teams that finished in last place. All three were also on playoff teams. I would say it's safe to assume that if they played for the same team they wouldn't have been able play. Also, it should be noted that these are the only three Selks that played on NHL teams in that time period. For the other two, I am not sure if their teams were playoff or not. One of those two was the first Sel\"}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "synthetic_dataset_raw[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esempio di output post-processato"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'label': 1,\n",
       " 'label_name': 'rec.sport.hockey',\n",
       " 'prompt': 'The Selke candidate forwards main purpose on a shift is to prevent goals from being scored not to score them. When Lemieux or Gilmour play their number one purpose is to score defence is secondary especially considering the line that plays against them is probably a defensive one. That is why they are not Selke candidates. Someone posted something about this assumption being lost in translation it was a few months ago .',\n",
       " 'text': \"The Selke candidate forwards main purpose on a shift is to prevent goals from being scored not to score them. When Lemieux or Gilmour play their number one purpose is to score defence is secondary especially considering the line that plays against them is probably a defensive one. That is why they are not Selke candidates. Someone posted something about this assumption being lost in translation it was a few months ago. I think it is a good idea to look at the numbers and see if it holds true. The first thing to note is that the Selkes are a very small sample size. The last time the Canucks were in the playoffs they had a total of 5 players on the team that were Selkendigs. This year they have only had 3. So it would be a stretch to say that they would have a higher percentage of Selkie's than the last few years. However, the fact that there are only 3 Selkies in this group is not a huge surprise. It is also not surprising that only one of them was on an NHL team. There are many reasons why a player would not be on their team, but the most common is because they were injured. In the case of the three players that are on this list, they all played in a playoff game. They were all injured and were not able to play in their playoff games. If you look back at their numbers, you will see that all three of these players were on teams that finished in last place. All three were also on playoff teams. I would say it's safe to assume that if they played for the same team they wouldn't have been able play. Also, it should be noted that these are the only three Selks that played on NHL teams in that time period. For the other two, I am not sure if their teams were playoff or not. One of those two was the first Sel\"}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "synthetic_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "synthetic_dataset_raw_df = pd.DataFrame(synthetic_dataset_raw) # Convert the list of dictionaries to a dataframe\n",
    "synthetic_dataset_df = pd.DataFrame(synthetic_dataset) # Convert the list of dictionaries to a dataframe\n",
    "synthetic_dataset_raw_df.to_csv('synthetic_dataset_raw_test.csv', index=False) # Save the raw generated samples to a CSV file\n",
    "synthetic_dataset_df.to_csv('synthetic_dataset_test.csv', index=False) # Save the processed generated samples to a CSV file"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
